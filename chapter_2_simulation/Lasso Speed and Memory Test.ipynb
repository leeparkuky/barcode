{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db0aaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /root/barcode/.venv/lib/python3.10/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /root/barcode/.venv/lib/python3.10/site-packages (from datasets) (12.0.0)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /root/barcode/.venv/lib/python3.10/site-packages (from datasets) (2.0.1)\n",
      "Collecting requests>=2.19.0 (from datasets)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /root/barcode/.venv/lib/python3.10/site-packages (from datasets) (4.65.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /root/barcode/.venv/lib/python3.10/site-packages (from datasets) (2023.5.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /root/barcode/.venv/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/barcode/.venv/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting charset-normalizer<4.0,>=2.0 (from aiohttp->datasets)\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: filelock in /root/barcode/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/barcode/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.8.0)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->datasets)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/barcode/.venv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.2)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.19.0->datasets)\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/barcode/.venv/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/barcode/.venv/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /root/barcode/.venv/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /root/barcode/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, multidict, idna, frozenlist, dill, charset-normalizer, certifi, attrs, async-timeout, yarl, requests, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.1.0 certifi-2023.7.22 charset-normalizer-3.3.2 datasets-2.14.6 dill-0.3.7 frozenlist-1.4.0 huggingface-hub-0.18.0 idna-3.4 multidict-6.0.4 multiprocess-0.70.15 requests-2.31.0 xxhash-3.4.1 yarl-1.9.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3f30b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/barcode/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.append('/root/barcode/')\n",
    "sys.path.append('../')\n",
    "from BarcodeScanner import tree_and_clustering, base_barcode\n",
    "from itertools import product, combinations\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import timeit\n",
    "from datasets import Dataset\n",
    "\n",
    "def gen_X(num_var: int, sample_size : int):\n",
    "    data_dictionary = {}\n",
    "    for i in range(num_var):\n",
    "        var_name = \"x\" + f\"{i + 1}\"\n",
    "        data_dictionary[var_name] = list(np.random.binomial(1, .5, sample_size))\n",
    "    return pd.DataFrame(data_dictionary)\n",
    "\n",
    "def gen_full_X(num_var: int, sample_size :int):\n",
    "    raw_X = gen_X(num_var = num_var, sample_size = sample_size)\n",
    "    colnames = raw_X.columns\n",
    "    for k in range(2, len(colnames)+ 1):\n",
    "        interaction_generator = combinations(colnames, k)\n",
    "        for interaction_tuple in interaction_generator:\n",
    "            new_colname = \"*\".join(interaction_tuple)\n",
    "            raw_X[new_colname] = raw_X[list(interaction_tuple)].apply(np.prod, axis = 1)\n",
    "    return raw_X\n",
    "\n",
    "def gen_barcode_dataloader(num_var:int, sample_size:int):\n",
    "    raw_X = gen_X(num_var = num_var, sample_size = sample_size)\n",
    "    colnames = [\n",
    "        f\"x{i+1}\" for i in range(num_var)\n",
    "    ]\n",
    "    from datasets import Dataset\n",
    "\n",
    "    dataset = Dataset.from_pandas(raw_X)\n",
    "    def gen_z(examples):\n",
    "        example_list = [examples[x] for x in colnames]\n",
    "        df = pd.DataFrame(zip(*example_list), columns= colnames)\n",
    "        barcodes = base_barcode.gen_barcode(df).reshape(-1).tolist()\n",
    "        y = df.apply(lambda seq: 1 + seq.x1 + seq.x2 + seq.x1*seq.x3 + np.random.normal(), axis = 1).tolist()\n",
    "        return {\"z\": barcodes, \"y\":y}\n",
    "    dataset = dataset.map(gen_z, batched = True, remove_columns=colnames)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12b194bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def L(p):\n",
    "    all_sets = list(set(product([0,1], repeat = p))); all_sets.sort()\n",
    "    return np.array([base_barcode.barcode_to_beta(x) for x in all_sets]).astype(np.int8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc57ecb",
   "metadata": {},
   "source": [
    "### Experiment 1: Memory consumption on entire pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b9667d",
   "metadata": {},
   "source": [
    "#### linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b1709334",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, n = 5, 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "875e4a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "X =gen_full_X(p, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4c3d7079",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['y'] = np.random.normal(size = n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4c574ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('sample_dataset.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "914abb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.84959983825684"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_size = os.path.getsize('sample_dataset.csv')\n",
    "file_size/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ec26a426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244.1407470703125"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('sample_dataset.csv'\n",
    ").__sizeof__()/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8d5396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.normal(size = n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0de1aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c0b493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad856213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2f9ec2",
   "metadata": {},
   "source": [
    "#### neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de664e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, n = 5, 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ebf06af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 24446.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "input_dataset = gen_barcode_dataloader(p, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4d06c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = input_dataset.train_test_split(test_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fea40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(input_dataset['train'], batch_size=126, shuffle=True)\n",
    "test_dataloader = DataLoader(input_dataset['test'], batch_size=126, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ba2066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Lasso_Barcode(nn.Module):\n",
    "    def __init__(self, num_variable):\n",
    "        super().__init__()\n",
    "        barcode_size = 2**num_variable\n",
    "        embedding_weights = torch.from_numpy(L(num_variable).astype(float))\n",
    "        self.embedding = nn.Embedding(barcode_size, barcode_size)\n",
    "        self.embedding.weight = nn.Parameter(embedding_weights.to(torch.float32), requires_grad = False)\n",
    "        self.linear = nn.Linear(barcode_size, 1, bias = False, dtype = torch.float32)\n",
    "\n",
    "    def l1_reg(self):\n",
    "        return torch.abs(self.linear.weight).sum()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.linear(x)\n",
    "        return x, self.l1_reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae60506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso_Barcode(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53a3c3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(lasso.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49329175",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4b5a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "val_loss = np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    lasso.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_tensor = batch['z']\n",
    "        output_tensor = batch['y'].to(torch.float32)\n",
    "        output_tensor = torch.reshape(output_tensor, (-1, ))\n",
    "#         optimizer.zero_grad()\n",
    "        assert input_tensor.size() == output_tensor.size()\n",
    "    \n",
    "\n",
    "        outputs, l1_reg = lasso(input_tensor)\n",
    "        # loss = criterion(outputs, output_feature)\n",
    "        \n",
    "        loss = criterion(outputs, output_tensor) + alpha * l1_reg  # Total loss with L1 regularization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        lasso.eval()\n",
    "        losses = []\n",
    "        for batch in test_dataloader:\n",
    "            input_tensor = batch['z']\n",
    "            output_tensor = batch['y'].to(torch.float32)\n",
    "            output_tensor = torch.reshape(output_tensor, (-1, ))\n",
    "            assert input_tensor.size() == output_tensor.size()\n",
    "            outputs, l1_reg = lasso(input_tensor)\n",
    "            loss = criterion(outputs, output_tensor) + alpha * l1_reg  # Total loss with L1 regularization\n",
    "            losses.append(loss.item())\n",
    "        current_val_loss = np.mean(losses)\n",
    "        if val_loss > current_val_loss:\n",
    "            val_loss = current_val_loss\n",
    "        else:\n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4d6adc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.1231,  0.6960,  0.7544,  0.6514,  0.7150,  0.7488, -0.3390, -0.3929,\n",
       "         -0.3170, -0.4057, -0.2643,  0.0650, -0.4015, -0.1308, -0.2917,  0.0311,\n",
       "          0.1803,  0.1104, -0.1903,  0.1278, -0.0584, -0.0703, -0.2474,  0.0523,\n",
       "         -0.2329, -0.1193,  0.0064, -0.0453,  0.0490, -0.1698, -0.1807,  0.0346]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.linear.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e3703",
   "metadata": {},
   "source": [
    "This time, please use the \"from memory_profiler import profile\" and profile decorator and run the python script and manually check the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a203dece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map (num_proc=10): 100%|███| 1000000/1000000 [00:07<00:00, 134004.44 examples/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [04:32<00:00, 27.29s/it]\n",
      "Filename: /root/barcode/chapter_2_simulation/lasso_memory_simulation_file.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "   123    627.9 MiB    627.9 MiB           1   @profile\n",
      "   124                                         def pipeline(p, n, input_dataset):\n",
      "   125    627.9 MiB      0.0 MiB           1       device = torch.device('cuda')\n",
      "   126    647.5 MiB     19.6 MiB           1       input_dataset = input_dataset.train_test_split(test_size = .2)\n",
      "   127                                         \n",
      "   128    647.5 MiB      0.0 MiB           1       train_dataloader = DataLoader(input_dataset['train'], batch_size=2**13, shuffle=True)\n",
      "   129    647.5 MiB      0.0 MiB           1       test_dataloader = DataLoader(input_dataset['test'], batch_size=2**14, shuffle=True)\n",
      "   130                                         \n",
      "   131                                         \n",
      "   132    750.7 MiB    103.2 MiB           1       lasso = Lasso_Barcode(p).to(device)\n",
      "   133    752.6 MiB      1.9 MiB           1       optimizer = optim.Adam(lasso.parameters(), lr=0.001)\n",
      "   134    752.6 MiB      0.0 MiB           1       criterion = nn.MSELoss()\n",
      "   135    752.6 MiB      0.0 MiB           1       num_epochs = 10\n",
      "   136    752.6 MiB      0.0 MiB           1       val_loss = np.inf\n",
      "   137    995.0 MiB      0.0 MiB          11       for epoch in tqdm(range(num_epochs)):\n",
      "   138                                                 # Forward pass\n",
      "   139    995.0 MiB      0.0 MiB          10           alpha = 0.3\n",
      "   140    995.0 MiB      0.0 MiB          10           lasso.train()\n",
      "   141    995.0 MiB     24.8 MiB         990           for batch in train_dataloader:\n",
      "   142    995.0 MiB      0.0 MiB         980               input_tensor = batch['z']\n",
      "   143    995.0 MiB      0.0 MiB         980               output_tensor = batch['y'].to(torch.float32)\n",
      "   144    995.0 MiB      0.0 MiB         980               output_tensor = torch.reshape(output_tensor, (-1, 1))\n",
      "   145                                         \n",
      "   146    995.0 MiB      0.0 MiB         980               if device:\n",
      "   147    995.0 MiB      0.0 MiB         980                   input_tensor = input_tensor.to(device)\n",
      "   148    995.0 MiB      0.0 MiB         980                   output_tensor = output_tensor.to(device)\n",
      "   149                                         \n",
      "   150    995.0 MiB      0.0 MiB         980               optimizer.zero_grad()\n",
      "   151    995.0 MiB     87.7 MiB         980               outputs, l1_reg = lasso(input_tensor)\n",
      "   152                                                     # loss = criterion(outputs, output_feature)\n",
      "   153                                         \n",
      "   154    995.0 MiB     43.5 MiB         980               loss = criterion(outputs, output_tensor) + alpha * l1_reg  # Total loss with L1 regularization\n",
      "   155    995.0 MiB     31.0 MiB         980               loss.backward()\n",
      "   156    995.0 MiB     55.1 MiB         980               optimizer.step()\n",
      "   157                                         \n",
      "   158    995.0 MiB      0.0 MiB          10           if epoch % 10 == 0:\n",
      "   159    994.0 MiB      0.3 MiB           1               current_val_loss = evaluate(test_dataloader, lasso, device = device)\n",
      "   160    994.0 MiB      0.0 MiB           1               if val_loss > current_val_loss:\n",
      "   161    994.0 MiB      0.0 MiB           1                   val_loss = current_val_loss\n",
      "   162                                                     else:\n",
      "   163                                                         break\n",
      "   164                                                         \n",
      "   165    995.0 MiB      0.0 MiB           1       return lasso\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python lasso_memory_simulation_file.py -p 5 -n 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b05f666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /root/barcode/chapter_2_simulation/linear_model_simulation_file.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    46    247.0 MiB    247.0 MiB           1   @profile\n",
      "    47                                         def pipeline(p, n):\n",
      "    48    715.4 MiB    468.4 MiB           1       df = pd.read_csv('sample_dataset.csv')\n",
      "    49    952.3 MiB    236.9 MiB           1       X = df.loc[:, df.columns.str.contains('x')].to_numpy()\n",
      "    50    952.3 MiB      0.0 MiB           1       y = df.y.to_numpy().reshape(-1,1)\n",
      "    51    952.3 MiB      0.0 MiB           1       reg = LinearRegression()\n",
      "    52    967.1 MiB     14.8 MiB           1       reg.fit(X, y)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python linear_model_simulation_file.py -p 5 -n 1_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1240ce29",
   "metadata": {},
   "source": [
    "### Experiment 2: Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a8b0f8",
   "metadata": {},
   "source": [
    "Choose the proper early stopping criteria for neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

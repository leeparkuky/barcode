{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# sys.path.append('/root/barcode/')\n",
    "sys.path.append('../')\n",
    "\n",
    "from BarcodeScanner import tree_and_clustering, base_barcode\n",
    "from itertools import product, combinations\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "# from interaction_permutation_importance import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gen_X(num_var: int, sample_size : int):\n",
    "    data_dictionary = {}\n",
    "    for i in range(num_var):\n",
    "        var_name = \"x\" + f\"{i + 1}\"\n",
    "        data_dictionary[var_name] = list(np.random.binomial(1, .5, sample_size))\n",
    "    return pd.DataFrame(data_dictionary)\n",
    "\n",
    "sys.path.append('/root/barcode/')\n",
    "from BarcodeScanner import tree_and_clustering, base_barcode\n",
    "from importance_score import f_test_result\n",
    "import scipy\n",
    "from scipy.stats import f\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics      import r2_score\n",
    "\n",
    "import numpy as np; import pandas as pd\n",
    "from typing import List, Union\n",
    "\n",
    "# itertools\n",
    "from itertools import combinations\n",
    "\n",
    "from utils import *\n",
    "\n",
    "\n",
    "class one_way_ANOVA(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, df, response_var_name =None):\n",
    "        if response_var_name:\n",
    "            try:\n",
    "                df[response_var_name]\n",
    "                self.response_var_name = response_var_name\n",
    "            except:\n",
    "                self.response_var_name = df.columns.tolist()[-1]\n",
    "        else:\n",
    "            self.response_var_name = df.columns.tolist()[-1]\n",
    "        self.independet_var_name = df.columns[~df.columns.str.contains(self.response_var_name)].tolist()\n",
    "        self.segment_means = df.groupby(self.independet_var_name)[self.response_var_name].mean().reset_index()\n",
    "        self.segment_means.columns= self.segment_means.columns.tolist()[:-1] + ['y_hat']\n",
    "        output = df.merge(self.segment_means, how = 'left', on = self.independet_var_name)\n",
    "        y = output[self.response_var_name].to_numpy().reshape(-1)\n",
    "        y_pred = output['y_hat'].to_numpy().reshape(-1)\n",
    "        self._rsq = r2_score(y, y_pred)\n",
    "        self.sse  = np.square(y - y_pred).sum()\n",
    "        self.MSE_ = self.sse/(y.shape[0])\n",
    "        output['sq'] = (output.y_hat - output[self.response_var_name])**2\n",
    "        summary = output.groupby(self.independet_var_name).agg({\"y_hat\": np.mean, \"sq\": [lambda x: x.sum()/x.count(), 'count']}).reset_index()\n",
    "        summary.columns = self.independet_var_name + ['group_means','mse','count']\n",
    "        self._summary_table = summary\n",
    "        return self\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def score(self):\n",
    "        if hasattr(self, '_rsq'):\n",
    "            return self._rsq\n",
    "        else:\n",
    "            raise AttributeError(\"fit the estimator first\")\n",
    "    \n",
    "    @property\n",
    "    def MSE(self):\n",
    "        if hasattr(self, \"MSE_\"):\n",
    "            return self.MSE_\n",
    "        else:\n",
    "            raise AttributeError(\"fit the estimator first\")\n",
    "class anova_clustering(base_barcode):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__(X, y)\n",
    "        self.clean_data()\n",
    "        self.estimator = one_way_ANOVA()\n",
    "        self.fit()\n",
    "\n",
    "    def clean_data(self):\n",
    "        full_df = self.X.copy()\n",
    "        full_df['y'] = self.y.reshape(-1)\n",
    "        sorted_index = full_df.sort_values(full_df.columns.tolist()[:-1]).index.tolist()\n",
    "        self.full_df = full_df.loc[sorted_index, :].reset_index(drop = True)\n",
    "        self.barcode_df = pd.DataFrame(zip(self.barcode.reshape(-1), self.y.reshape(-1)), columns = ['z','y']).loc[sorted_index, :].reset_index(drop = True)\n",
    "        self.X = full_df.iloc[:, :-1]\n",
    "        self.y = full_df.y.to_numpy().reshape(-1,1);\n",
    "        self.num_full_var = 2**self.X.shape[1]\n",
    "        self.num_main_var = self.X.shape[1]\n",
    "        del sorted_index\n",
    "        del full_df\n",
    "        \n",
    "    def fit(self):   \n",
    "        self.estimator.fit(self.full_df)\n",
    "        self._summary_table = self.estimator._summary_table\n",
    "        self._fit = True\n",
    "\n",
    "    def gen_clustering_summary_table(self, num_clusters):\n",
    "        original_summary = self.summary_table.copy()\n",
    "        original_summary = original_summary.drop(['group_means','mse', 'count'], axis = 1)\n",
    "        original_summary['y_pred'] = self.get_projected_mu_hat(num_clusters)\n",
    "        full_df = self.full_df.copy()\n",
    "        full_df = full_df.merge(original_summary, on = self.estimator.independet_var_name, how = 'left')\n",
    "        return full_df\n",
    "    \n",
    "    def metric_after_clustering(self, num_clusters, metric = r2_score):\n",
    "        full_df = self.gen_clustering_summary_table(num_clusters)\n",
    "        return metric(full_df.y, full_df.y_pred)\n",
    "\n",
    "\n",
    "\n",
    "    @property\n",
    "    def summary_table(self):\n",
    "        if hasattr(self, \"_summary_table\"):\n",
    "            pass\n",
    "        else:\n",
    "            self._summary_table = self.predict_from_training()\n",
    "        return self._summary_table\n",
    "    \n",
    "    @property\n",
    "    def init_mu_hat(self):\n",
    "        if hasattr(self, \"_init_mu_hat\"):\n",
    "            pass\n",
    "        else:\n",
    "            self._init_mu_hat = self.summary_table.group_means.tolist()\n",
    "            self._init_mu_hat = np.array(self._init_mu_hat).reshape(-1)\n",
    "        return self._init_mu_hat\n",
    "    \n",
    "    @property\n",
    "    def init_mu_var(self):\n",
    "        if hasattr(self, \"_init_mu_var\"):\n",
    "            pass\n",
    "        else:\n",
    "            self._init_mu_var = np.diag(self.estimator.MSE/self.summary_table['count'])\n",
    "        return self._init_mu_var\n",
    "    \n",
    "    @property\n",
    "    def init_beta_hat(self):\n",
    "        if hasattr(self, \"_init_beta_hat\"):\n",
    "            pass\n",
    "        else:\n",
    "            self._init_beta_hat = self.L_inv @ self.init_mu_hat\n",
    "        return self._init_beta_hat\n",
    "\n",
    "    @property\n",
    "    def init_beta_var(self):\n",
    "        if hasattr(self, \"_init_beta_var\"):\n",
    "            pass\n",
    "        else:\n",
    "            self._init_beta_var = self.L_inv @ self.init_mu_var @ self.L_inv.T\n",
    "        return self._init_beta_var\n",
    "\n",
    "    @property\n",
    "    def clustering_init_kwargs(self):\n",
    "        summary = self.summary_table\n",
    "        kwargs = {\"means\": summary.group_means.tolist(), \"variances\": summary.mse.tolist(), \"sample_sizes\": summary['count'].tolist()}\n",
    "        return kwargs\n",
    "    \n",
    "    @property\n",
    "    def init_cluster_idx(self):\n",
    "        if hasattr(self, \"_init_cluster_idx\"):\n",
    "            pass\n",
    "        else:\n",
    "            sample_sizes = self.clustering_init_kwargs['sample_sizes']\n",
    "            clusters = []\n",
    "            last = 0\n",
    "            for n in sample_sizes:\n",
    "                clusters.append([x for x in range(last, n+last)])\n",
    "                last += n\n",
    "            self._init_cluster_idx = clusters\n",
    "        return self._init_cluster_idx\n",
    "\n",
    "    @property\n",
    "    def init_pdist(self):\n",
    "        if hasattr(self, \"_init_pdist\"):\n",
    "            pass\n",
    "        else:\n",
    "            self._init_pdist = self.pairwise_distances_from_means_variances(**self.clustering_init_kwargs)\n",
    "        return self._init_pdist\n",
    "\n",
    "    def cluster(self, n_clusters, save = True):\n",
    "        self.last_n_clusters = n_clusters\n",
    "        pdist = self.init_pdist.copy()\n",
    "        init_cluster_idx = self.init_cluster_idx.copy()\n",
    "        result = self.agglomerative_clustering(pairwise_distances = pdist, n_clusters = n_clusters, clusters = init_cluster_idx)\n",
    "        cluster_idx = result[0]\n",
    "        final_pdist = result[1]\n",
    "        cluster_df = {}\n",
    "        for cluster_id, cluster_index in enumerate(cluster_idx):\n",
    "            cluster_id_name = f\"cluster_{cluster_id}\"\n",
    "            cluster_df[cluster_id_name] = self.full_df.loc[cluster_index,:].copy()\n",
    "            cluster_df[cluster_id_name] = cluster_df[cluster_id_name].groupby(self.original_columns).agg(np.mean).reset_index()\n",
    "            cluster_df[cluster_id_name]['barcode'] = self.gen_barcode((cluster_df[cluster_id_name].loc[:, self.original_columns]))\n",
    "        if save:\n",
    "            self._latest_cluster_dfs = cluster_df\n",
    "        else:\n",
    "            self._latest_cluster_dfs = None\n",
    "        \n",
    "        return {\"cluster\": cluster_df, \"final_pdist\": final_pdist}\n",
    "\n",
    "    def gen_mu_contrast_from_cluster(self, n_clusters, use_latest_cluster_df = True):\n",
    "        if use_latest_cluster_df:\n",
    "            if hasattr(self, '_latest_cluster_dfs'):\n",
    "                if (self._latest_cluster_dfs != None) & (self.last_n_clusters == n_clusters):\n",
    "                    cluster_df_list = self._latest_cluster_dfs\n",
    "        try:\n",
    "            cluster_df_list\n",
    "        except:\n",
    "            cluster_result = self.cluster(n_clusters = n_clusters)\n",
    "            cluster_df_list = cluster_result['cluster']\n",
    "            \n",
    "        contrast_matrix = 0\n",
    "        for cluster_name, cluster_df in cluster_df_list.items():\n",
    "            if cluster_df.shape[0] > 1:\n",
    "                contrast = np.zeros((cluster_df.shape[0]-1, 2**len(self.original_columns)))\n",
    "                for i, row in enumerate(contrast):\n",
    "                    row[cluster_df.barcode[0]] = 1\n",
    "                    row[cluster_df.barcode[i+1]] = -1\n",
    "                    contrast[i] = row\n",
    "                if isinstance(contrast_matrix, np.ndarray):\n",
    "                    contrast_matrix = np.concatenate([contrast_matrix, contrast], axis = 0)\n",
    "                else:\n",
    "                    contrast_matrix = contrast\n",
    "            else:\n",
    "                pass\n",
    "        return contrast_matrix\n",
    "    \n",
    "    def gen_projection_matrix_mu(self, n_clusters):\n",
    "        C = self.gen_mu_contrast_from_cluster(n_clusters)\n",
    "        projection_matrix = C.T @ np.linalg.inv(C @ C.T) @ C\n",
    "        projection_matrix = np.identity(C.shape[1]) - projection_matrix\n",
    "        return projection_matrix\n",
    "    \n",
    "    def get_projected_mu_hat(self, n_clusters):\n",
    "        mu = self.init_mu_hat\n",
    "        proj_mu = self.gen_projection_matrix_mu(n_clusters = n_clusters) @ mu\n",
    "        return proj_mu\n",
    "    \n",
    "    def get_projected_mu_hat_var(self, n_clusters):\n",
    "        mu_var = self.init_mu_var\n",
    "        proj = self.gen_projection_matrix_mu(n_clusters = n_clusters)\n",
    "        proj_mu_var = proj @ mu_var @ proj.T\n",
    "        return proj_mu_var\n",
    "    \n",
    "    def get_projected_beta_hat(self, n_clusters):\n",
    "        proj_mu = self.get_projected_mu_hat(n_clusters = n_clusters)\n",
    "        proj_beta = self.L_inv @ proj_mu\n",
    "        return proj_beta\n",
    "    \n",
    "    def get_projected_beta_hat_var(self, n_clusters):\n",
    "        mu_var = self.get_projected_mu_hat_var(n_clusters)\n",
    "        return self.L_inv @ mu_var @ self.L_inv.T\n",
    "\n",
    "    \n",
    "    def ward_linkage(self, pairwise_distances, clusters, merge_indices):\n",
    "        clusters = clusters.copy()\n",
    "        i, j = merge_indices\n",
    "        cluster_i = clusters[i]\n",
    "        cluster_j = clusters[j]\n",
    "        n_i = len(cluster_i)\n",
    "        n_j = len(cluster_j)\n",
    "        n = pairwise_distances.shape[1]\n",
    "        new_distances = pairwise_distances.copy()\n",
    "        new_distances = np.delete(new_distances, merge_indices, axis = 0)\n",
    "        new_distances = np.delete(new_distances, merge_indices, axis = 1)\n",
    "        new_distances = np.append(new_distances, np.zeros((1, new_distances.shape[1])), axis = 0)\n",
    "        new_distances = np.append(new_distances,  np.zeros((new_distances.shape[0], 1)), axis = 1)\n",
    "        clusters.append(cluster_i + cluster_j)\n",
    "        clusters.remove(cluster_i)\n",
    "        clusters.remove(cluster_j)\n",
    "        col = 0\n",
    "        \n",
    "        for k in range(n):  # Subtract 2 because we've already added a row and column\n",
    "            if k != i and k != j:\n",
    "                n_k = len(clusters[col])\n",
    "                n_all = n_i + n_j + n_k\n",
    "                dist_ik = pairwise_distances[i, k]*(n_k + n_i)/n_all\n",
    "                dist_jk = pairwise_distances[j, k]*(n_j + n_k)/n_all\n",
    "                dist_ij = pairwise_distances[i, j]*(n_k)/n_all\n",
    "                new_dist = dist_ik + dist_jk - dist_ij\n",
    "                # Add other linkage methods here if needed\n",
    "                \n",
    "                new_distances[-1, col] = new_dist\n",
    "                new_distances[col, -1] = new_dist\n",
    "                col += 1\n",
    "\n",
    "        return new_distances, clusters\n",
    "    \n",
    "    def gen_reduced_clustering(self, n_clusters):\n",
    "        return self.agglomerative_clustering(pairwise_distances= self.init_pdist, n_clusters = n_clusters)\n",
    "\n",
    "    def agglomerative_clustering(self, pairwise_distances, n_clusters, clusters = None, estimator = None):\n",
    "        n = pairwise_distances.shape[0]\n",
    "        if clusters:\n",
    "            pass\n",
    "        else:\n",
    "            clusters = [[i] for i in range(n)]\n",
    "        \n",
    "        assert len(clusters) == n\n",
    "        \n",
    "        for k in range(n - n_clusters):\n",
    "            min_dist = np.inf\n",
    "            merge_indices = None\n",
    "            \n",
    "            for i in range(len(clusters)):\n",
    "                for j in range(i + 1, len(clusters)):\n",
    "                    dist = pairwise_distances[i, j]\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        merge_indices = (i, j)\n",
    "            \n",
    "            if merge_indices:\n",
    "                pairwise_distances, clusters = self.ward_linkage(pairwise_distances, clusters, merge_indices)\n",
    "        \n",
    "        return clusters, pairwise_distances\n",
    "    \n",
    "    def pairwise_distances_from_means_variances(self, means, variances, sample_sizes):\n",
    "        num_clusters = len(means)\n",
    "        pairwise_distances = np.zeros((num_clusters, num_clusters))\n",
    "        \n",
    "        for i in range(num_clusters):\n",
    "            for j in range(i + 1, num_clusters):\n",
    "                n_i = sample_sizes[i]\n",
    "                n_j = sample_sizes[j]\n",
    "                mean_i = means[i]\n",
    "                mean_j = means[j]\n",
    "                var_i = variances[i]\n",
    "                var_j = variances[j]\n",
    "                \n",
    "                # Calculate the pairwise distance using Ward linkage formula\n",
    "                numerator = (n_i * n_j / (n_i + n_j)) * (mean_i - mean_j)**2\n",
    "                denominator = np.sqrt((n_i * var_i + n_j * var_j) / (n_i + n_j))\n",
    "                \n",
    "                pairwise_distances[i, j] = np.sqrt(numerator / denominator)\n",
    "                pairwise_distances[j, i] = pairwise_distances[i, j]\n",
    "        \n",
    "        return pairwise_distances\n",
    "\n",
    "    def contrast_generator(self, max_variables = None):\n",
    "        from itertools import product\n",
    "        from scipy.sparse import vstack\n",
    "        all_interaction_betas = scipy.sparse.eye(self.num_full_var - self.num_main_var -1, self.num_full_var, k = self.num_main_var + 1)\n",
    "        betas_in_test = product([False, True], repeat = self.num_full_var - self.num_main_var -1)\n",
    "        \n",
    "        def get_new_contrasts(beta):\n",
    "            vstacks = []\n",
    "            for i, b in enumerate(beta):\n",
    "                if b:\n",
    "                    vstacks.append(all_interaction_betas.getrow(i))\n",
    "            return vstack(vstacks).astype(np.uint8)\n",
    "        if max_variables is not None:\n",
    "            for beta in betas_in_test:\n",
    "                if sum(beta) and (sum(beta) <= max_variables):\n",
    "                    yield get_new_contrasts(beta)\n",
    "        else:\n",
    "            for beta in betas_in_test:\n",
    "                if sum(beta):\n",
    "                    yield get_new_contrasts(beta)\n",
    "\n",
    "\n",
    "    @staticmethod    \n",
    "    def partial_f_test(contrast, L_inv, cell_means, cell_means_covariance, sample_size):\n",
    "        assert contrast.shape[0] < contrast.shape[1]\n",
    "        assert contrast.shape[1] == L_inv.shape[0]\n",
    "        assert L_inv.shape[0] == L_inv.shape[1]\n",
    "        assert cell_means_covariance.shape == L_inv.shape\n",
    "        from numpy.linalg import inv\n",
    "        nu_1 = contrast.shape[0]\n",
    "        nu_2 = sample_size - L_inv.shape[0]\n",
    "        contrast = contrast.toarray()\n",
    "\n",
    "        LC = contrast @ L_inv.T\n",
    "        mu = LC @ cell_means\n",
    "        var = LC @ cell_means_covariance @ LC.T\n",
    "        try:\n",
    "            f_value = mu.T @ inv(var) @ mu\n",
    "            f_value /= nu_1\n",
    "        except:\n",
    "            f_value = 0\n",
    "        \n",
    "        return f_test_result(nu_1, nu_2, f_value)\n",
    "    \n",
    "    def gen_importance_score(self, num_clusters, filename = 'importance_score.csv', dir = os.getcwd(), max_variables = None):\n",
    "        cell_means = self.get_projected_mu_hat(num_clusters)\n",
    "        covariance = self.get_projected_mu_hat_var(num_clusters)\n",
    "        kwargs = {\"L_inv\": self.L_inv, \"cell_means\": cell_means, \"cell_means_covariance\": covariance, \"sample_size\": self.X.shape[0]}\n",
    "\n",
    "        from csv import writer\n",
    "        from tqdm import tqdm\n",
    "        with open(os.path.join(dir, filename), 'w') as f:\n",
    "            csv_writer = writer(f)\n",
    "            csv_writer.writerow(self.beta_names + ['score'])\n",
    "            contrasts =  self.contrast_generator(max_variables = max_variables)\n",
    "            from math import comb\n",
    "            if max_variables:\n",
    "                total_iter = sum([comb(self.num_full_var - self.num_main_var -1, x) for x in range(1, max_variables +1)])\n",
    "            else:\n",
    "                total_iter = sum([comb(self.num_full_var - self.num_main_var -1, x) for x in range(1,self.num_full_var - self.num_main_var)])\n",
    "            for contrast in tqdm(contrasts, total = total_iter):\n",
    "                result = self.partial_f_test(contrast = contrast, **kwargs)\n",
    "                score = result.cdf\n",
    "                row = contrast.sum(axis = 0).tolist()[0] + [score]\n",
    "                csv_writer.writerow(row)\n",
    "            else:\n",
    "                from joblib import Parallel, delayed\n",
    "                num_cores = os.cpu_count()\n",
    "                batch_size = 1000 * num_cores\n",
    "                partition_size = 1000\n",
    "                contrast_batch = []\n",
    "                for i, contrast in tqdm(enumerate(contrasts), total = total_iter):\n",
    "                    contrast_batch.append(contrast)\n",
    "                    if len(contrast_batch) % batch_size == 0:\n",
    "                        f_test_results = Parallel(n_jobs=num_cores)(delayed(self.partial_f_test)(c, **kwargs) for c in contrast_batch)\n",
    "                        f_test_results = [x.cdf for x in f_test_results]\n",
    "                        rows = [c.sum(axis = 0).tolist()[0] + [f] for c,f in zip(contrast_batch, f_test_results)]\n",
    "                        csv_writer.writerows(rows)\n",
    "                        del contrast_batch\n",
    "                        del rows\n",
    "                        del f_test_results\n",
    "                        contrast_batch = []\n",
    "                # process the remainder\n",
    "                if len(contrast_batch):\n",
    "                    f_test_results = Parallel(n_jobs=num_cores)(delayed(self.partial_f_test)(c, **kwargs) for c in contrast_batch)\n",
    "                    f_test_results = [x.cdf for x in f_test_results]\n",
    "                    rows = [c.sum(axis = 0).tolist()[0] + [f] for c,f in zip(contrast_batch, f_test_results)]\n",
    "                    csv_writer.writerows(rows)\n",
    "\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from dask import dataframe as dd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "@dataclass\n",
    "class f_test_result:\n",
    "    nu_1:int\n",
    "    nu_2:int\n",
    "    f_statistic: float\n",
    "    \n",
    "    @property\n",
    "    def p_value(self):\n",
    "        if hasattr(self, '_p_value'):\n",
    "            pass\n",
    "        else:\n",
    "            sdf = f.sf(self.f_statistic, self.nu_1, self.nu_2, loc=0, scale=1)\n",
    "            self._p_value = sdf\n",
    "        return self._p_value\n",
    "    \n",
    "    @property\n",
    "    def cdf(self):\n",
    "        if hasattr(self, '_cdf'):\n",
    "            pass\n",
    "        else:\n",
    "            self._cdf = 1 - self.p_value\n",
    "        return self._cdf\n",
    "from math import sqrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [0,0,0,0,0,0,1,1,1,1]\n",
    "x2 = [0,0,0,0,1,1,0,0,1,1]\n",
    "y = [-3.655697,-4.848736,-5.537694,-8.661518,-8.486865,-11.669568,-3.312871,-2.031480,-8.475096,-5.116814]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(zip(x1, x2, y), columns = ['x1','x2','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.655697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.848736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.537694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.661518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-8.486865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-11.669568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.312871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.031480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-8.475096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.116814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2          y\n",
       "0   0   0  -3.655697\n",
       "1   0   0  -4.848736\n",
       "2   0   0  -5.537694\n",
       "3   0   0  -8.661518\n",
       "4   0   1  -8.486865\n",
       "5   0   1 -11.669568\n",
       "6   1   0  -3.312871\n",
       "7   1   0  -2.031480\n",
       "8   1   1  -8.475096\n",
       "9   1   1  -5.116814"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode = anova_clustering(X = df.iloc[:, :-1], y = df.iloc[:, -1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -5.67591125, -10.0782165 ,  -2.6721755 ,  -6.795955  ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode.init_mu_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.67591125,  3.00373575, -4.40230525,  0.27852575])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode.init_beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5223245820265747"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode.estimator.MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63058115, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 1.26116229, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.26116229, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.26116229]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode.init_mu_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.63058115, -0.63058115, -0.63058115,  0.63058115],\n",
       "       [-0.63058115,  1.89174344,  0.63058115, -1.89174344],\n",
       "       [-0.63058115,  0.63058115,  1.89174344, -1.89174344],\n",
       "       [ 0.63058115, -1.89174344, -1.89174344,  4.41406802]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode.init_beta_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.96459681, 1.58583873, 1.58583873, 0.79291936],\n",
       "       [1.58583873, 1.58583873, 0.79291936, 0.79291936],\n",
       "       [1.58583873, 0.79291936, 1.58583873, 0.79291936],\n",
       "       [0.79291936, 0.79291936, 0.79291936, 0.79291936]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv(barcode.init_beta_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "X['1'] = 1\n",
    "X['x1x2'] = X.x1*X.x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[['1','x1','x2','x1x2']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(barcode.estimator.MSE * inv(X.T @ X)).round(4) == barcode.init_beta_var.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_beta = np.array([[0, 0, 1, 0],\n",
    "                   [0, 0, 1, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_mu = C_beta.dot(barcode.L.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1],\n",
       "       [0, 1, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-16.8741715],\n",
       "       [-10.0782165]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_mu.dot(barcode.init_mu_hat.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "K  = C_beta @ barcode.L_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_beta = np.array([[0, 0, 1, 0],\n",
    "                   [0, 0, 1, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_mu = C_beta @ barcode.L.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.63058115, -0.63058115, -0.63058115,  0.63058115],\n",
       "       [-0.63058115,  1.89174344,  0.63058115, -1.89174344],\n",
       "       [-0.63058115,  0.63058115,  1.89174344, -1.89174344],\n",
       "       [ 0.63058115, -1.89174344, -1.89174344,  4.41406802]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode.L_inv @ barcode.init_mu_var @ barcode.L_inv.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63058115, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 1.26116229, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.26116229, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.26116229]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode.L @ barcode.L_inv @ barcode.init_mu_var @ barcode.L_inv.T @ barcode.L.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63058115, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 1.26116229, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.26116229, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.26116229]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode.init_mu_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1,  1,  0,  0],\n",
       "       [-2,  2,  1, -1]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_mu @ barcode.L_inv.T @ barcode.L_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1,  1,  0,  0],\n",
       "       [-2,  2,  1, -1]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  1,  0],\n",
       "       [ 0,  0,  1, -1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_mu @ barcode.L_inv.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.63058115, -0.63058115, -0.63058115,  0.63058115],\n",
       "       [-0.63058115,  1.89174344,  0.63058115, -1.89174344],\n",
       "       [-0.63058115,  0.63058115,  1.89174344, -1.89174344],\n",
       "       [ 0.63058115, -1.89174344, -1.89174344,  4.41406802]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode.init_beta_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.63058115, -0.63058115, -0.63058115,  0.63058115],\n",
       "       [-0.63058115,  1.89174344,  0.63058115, -1.89174344],\n",
       "       [-0.63058115,  0.63058115,  1.89174344, -1.89174344],\n",
       "       [ 0.63058115, -1.89174344, -1.89174344,  4.41406802]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode.L_inv @ barcode.init_mu_var @ barcode.L_inv.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.79291936, -0.79291936],\n",
       "       [-0.79291936,  1.58583873]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv(C_mu @ barcode.init_mu_var @ C_mu.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.40230525, -4.680831  ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode.init_mu_hat.reshape(1, -1) @ K.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.40230525, -4.680831  ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode.init_beta_hat.reshape(1, -1) @ C_beta.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[117.15815897]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode.init_mu_hat.reshape(1, -1) @ C_mu.T @ inv(C_mu @ barcode.init_mu_var @ C_mu.T) @ C_mu @ barcode.init_mu_hat.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.98669012]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode.init_mu_hat.reshape(1, -1) @ K.T @ inv(K @ barcode.init_mu_var @ K.T) @ K @ barcode.init_mu_hat.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.98669012]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode.init_beta_hat.reshape(1, -1) @ C_beta.T @ inv(C_beta @ barcode.init_beta_var @ C_beta.T) @ C_beta @ barcode.init_beta_hat.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.11445163, -0.79291936],\n",
       "       [-0.79291936,  0.39645968]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv(C_beta @ barcode.init_beta_var @ C_beta.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1,  1,  0,  0],\n",
       "       [-2,  2,  1, -1]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.67591125,  3.00373575, -4.40230525,  0.27852575])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode.init_beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-16.8741715, -10.0782165]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode.init_mu_hat.reshape(1, -1) @ C_mu.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.40230525, -4.680831  ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode.init_mu_hat.reshape(1, -1) @ K.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
